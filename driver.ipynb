{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import lda\n",
    "import numpy as np\n",
    "import os\n",
    "from serial_lda_gibbs import LdaSampler\n",
    "from multicore_lda_gibbs import MulticoreLdaSampler\n",
    "import time\n",
    "\n",
    "pickle_filepath = 'baseline_data.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_reuters_dataset():\n",
    "    if not os.path.exists(pickle_filepath):\n",
    "        dump_reuters_dataset()\n",
    "    with open(pickle_filepath, 'r') as rfile:\n",
    "        X, vocab, titles = pickle.load(rfile)\n",
    "    return X, vocab, titles\n",
    "\n",
    "def dump_reuters_dataset():\n",
    "    X = lda.datasets.load_reuters()\n",
    "    vocab = lda.datasets.load_reuters_vocab()\n",
    "    titles = lda.datasets.load_reuters_titles()\n",
    "\n",
    "    with open(pickle_filepath, 'w') as wfile:\n",
    "        pickle.dump( (X, vocab, titles), wfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serial_gibbs(X, k, iters=50, log=True):\n",
    "    sampler = LdaSampler(k)\n",
    "    start = time.time()\n",
    "    for it, phi in enumerate(sampler.run(X, maxiter=iters)):\n",
    "        if log:\n",
    "            print \"Iteration\", it\n",
    "            print \"Likelihood\", sampler.loglikelihood()\n",
    "        else:\n",
    "            i = it\n",
    "    end = time.time()\n",
    "    print 'Completed %d iterations in %.3f seconds (serial)' % (iters, end - start)\n",
    "    return sampler\n",
    "\n",
    "\n",
    "def multicore_gibbs(X, k, p, iters=50, log=True):\n",
    "    sampler = MulticoreLdaSampler(k, p)\n",
    "    start = time.time()\n",
    "    for it, phi in enumerate(sampler.run(X, maxiter=iters)):\n",
    "        if log:\n",
    "            print \"Iteration\", it\n",
    "            print \"Likelihood\", sampler.loglikelihood()\n",
    "        else:\n",
    "            i = it\n",
    "    end = time.time()\n",
    "    print 'Completed %d iterations in %.3f seconds (P=%d)' % (iters, end - start, p)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/Bag+of+Words\n",
    "def load(dataset):\n",
    "    files = {\n",
    "        'nips': 'docword.nips.txt', \n",
    "        'nytimes': 'docwords.nytimes.txt',\n",
    "    }\n",
    "    if dataset == 'reuters':\n",
    "        return load_reuters_dataset()\n",
    "    elif dataset in files: \n",
    "        with open(files[dataset], 'r') as rfile:\n",
    "            lines = rfile.readlines()\n",
    "        n_documents = int(lines[0])\n",
    "        n_words = int(lines[1])\n",
    "        X = np.zeros((n_documents, n_words))\n",
    "        data = map(lambda s: map(int, s.split()), lines[3:])\n",
    "        for doc, word, count in data:\n",
    "            X[doc-1][word-1] = count\n",
    "        return X\n",
    "    else:\n",
    "        raise Exception ('Dataset %s not found' % dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled in 3.151 seconds\n",
      "Sampled in 3.230 seconds\n",
      "Sampled in 3.187 seconds\n",
      "Sampled in 3.118 seconds\n",
      "Sampled in 3.005 seconds\n",
      "Sampled in 3.075 seconds"
     ]
    }
   ],
   "source": [
    "# serial_gibbs(load('nips'), 10, 16)\n",
    "times = {}\n",
    "likelihoods = {}\n",
    "data = load_reuters_dataset()[0]\n",
    "k = 10\n",
    "iters=50\n",
    "\n",
    "sampler = serial_gibbs(data, k, iters=50, log=False)\n",
    "times[0] = np.mean(sampler.sample_times)\n",
    "likelihoods[0] = sampler.loglikelihood()\n",
    "for p in [1,2,4,8,16]:\n",
    "    sampler = multicore_gibbs(data, k, p,iters=50, log=False)\n",
    "    times[p] = np.mean(sampler.sample_times) + np.mean(sampler.update_times)\n",
    "    likelihoods[p] = sampler.loglikelihood()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
