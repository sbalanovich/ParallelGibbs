{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import lda\n",
    "import numpy as np\n",
    "import os\n",
    "from serial_lda_gibbs import LdaSampler\n",
    "from multicore_lda_gibbs import MulticoreLdaSampler\n",
    "import time\n",
    "\n",
    "pickle_filepath = 'baseline_data.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_reuters_dataset():\n",
    "    if not os.path.exists(pickle_filepath):\n",
    "        dump_reuters_dataset()\n",
    "    with open(pickle_filepath, 'r') as rfile:\n",
    "        X, vocab, titles = pickle.load(rfile)\n",
    "    return X, vocab, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serial_gibbs(X, k, iters=50):\n",
    "    sampler = LdaSampler(k)\n",
    "    start = time.time()\n",
    "    for it, phi in enumerate(sampler.run(X, maxiter=iters)):\n",
    "        print \"Iteration\", it\n",
    "        print \"Likelihood\", sampler.loglikelihood()\n",
    "    end = time.time()\n",
    "    print 'Completed %d iterations in %.3f seconds (serial)' % (iters, end - start)\n",
    "    \n",
    "def multicore_gibbs(X, k, p, iters=50):\n",
    "    sampler = MulticoreLdaSampler(k, p)\n",
    "    start = time.time()\n",
    "    for it, phi in enumerate(sampler.run(X, maxiter=iters)):\n",
    "        print \"Iteration\", it\n",
    "        # print \"Likelihood\", sampler.loglikelihood()\n",
    "    end = time.time()\n",
    "    print 'Completed %d iterations in %.3f seconds (P=%d)' % (iters, end - start, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X, vocab, titles = load_reuters_dataset()\n",
    "# multicore_gibbs(X, 10, 16 False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/Bag+of+Words\n",
    "def load(dataset):\n",
    "    files = {\n",
    "        'nips': 'docword.nips.txt', \n",
    "        'nytimes': 'docwords.nytimes.txt',\n",
    "    }\n",
    "    if dataset == 'reuters':\n",
    "        return load_reuters_dataset()\n",
    "    elif dataset in files:\n",
    "        with open(files[dataset], 'r') as rfile:\n",
    "            lines = rfile.readlines()\n",
    "        n_documents = int(lines[0])\n",
    "        n_words = int(lines[1])\n",
    "        X = np.zeros((n_documents, n_words))\n",
    "        data = map(lambda s: map(int, s.split()), lines[3:])\n",
    "        for doc, word, count in data:\n",
    "            X[doc-1][word-1] = count\n",
    "        return X\n",
    "    else:\n",
    "        raise Exception ('Dataset %s not found' % dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# serial_gibbs(load('nips'), 10, 16)\n",
    "multicore_gibbs(load('nips'), 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
